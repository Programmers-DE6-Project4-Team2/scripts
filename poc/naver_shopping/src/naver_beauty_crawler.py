import time
import json
import requests
from datetime import datetime
from typing import List, Dict, Optional
import pandas as pd
from urllib.parse import quote
from fake_useragent import UserAgent
import logging
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NaverShoppingCrawler:
    """
    ë„¤ì´ë²„ ì‡¼í•‘ì˜ GraphQL APIë¥¼ ì´ìš©í•˜ì—¬ ìƒí’ˆ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í´ë˜ìŠ¤
    """
    def __init__(self):
        self.session = requests.Session()
        self.ua = UserAgent() # User-Agent ë¡œí…Œì´ì…˜ì„ ìœ„í•œ ê°ì²´
        self.setup_session()
        
        # ë„¤ì´ë²„ ì‡¼í•‘ GraphQL API ê¸°ë³¸ ì •ë³´
        self.base_api_url = "https://veco-api.shopping.naver.com/window/api/v2/graphql"
        self.operation_name = "getPagedCards"
        # "ì¸ê¸°ìˆœ" ì •ë ¬ì— í•´ë‹¹í•˜ëŠ” GraphQL Persisted Query Hash ê°’
        self.hash_value = "db693844352af4739286d5394a31659cac8a1643795d5de3bea7064ba7d7fa45"
        
        # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ë° ì •ë ¬ ì„¤ì • (ì´ ì„¤ì •ì€ fetch_products_api í˜¸ì¶œ ì‹œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
        self.default_display_category_id = "20006491" # ë„¤ì´ë²„ ë·°í‹° ì „ì²´ ì¹´í…Œê³ ë¦¬
        self.default_sort_type = "DISPLAY_CATEGORY_GENDER_AGE_GROUP_F20" # ì¸ê¸°ìˆœ (20ëŒ€ ì—¬ì„± ê¸°ì¤€)

    def setup_session(self):
        """HTTP ì„¸ì…˜ì˜ ê¸°ë³¸ í—¤ë”ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
        headers = {
            'User-Agent': self.ua.random, # ë§¤ ìš”ì²­ë§ˆë‹¤ ë‹¤ë¥¸ User-Agent ì‚¬ìš© ê°€ëŠ¥ (fake_useragent ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì‹œ)
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Origin': 'https://shopping.naver.com',
            'Referer': 'https://shopping.naver.com/',
            'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="124", "Google Chrome";v="124"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-site',
            'Connection': 'keep-alive',
            # POST ìš”ì²­ ì‹œ Content-Typeì€ requests ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ json íŒŒë¼ë¯¸í„° ì‚¬ìš© ì‹œ ìë™ìœ¼ë¡œ ì„¤ì •í•´ì¤ë‹ˆë‹¤.
            # 'Content-Type': 'application/json',
        }
        self.session.headers.update(headers)
        logger.info("ì„¸ì…˜ í—¤ë” ì„¤ì • ì™„ë£Œ.")

    def _build_graphql_payload(self, page: int, page_size: int, display_category_id: str, sort_type: str) -> Dict:
        """
        GraphQL POST ìš”ì²­ì— í•„ìš”í•œ JSON í˜ì´ë¡œë“œ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        variablesì™€ extensionsëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        variables = {
            "isIncludeProductBenefit": False,
            "isIncludeProductDetail": False,
            "isIncludeWindowViewCount": False,
            "skip": False,
            "checkPromotionProduct": False,
            "params": {
                "page": page,
                "pageSize": page_size,
                "filterSoldOut": True, # í’ˆì ˆ ìƒí’ˆ í•„í„°ë§
                "useNonSubVertical": True,
                "displayCategoryId": display_category_id,
                "sort": sort_type
            }
        }
        
        extensions = {
            "persistedQuery": {
                "version": 1,
                "sha256Hash": self.hash_value
            }
        }
        
        return {
            "operationName": self.operation_name,
            "variables": variables, # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì „ë‹¬
            "extensions": extensions # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì „ë‹¬
        }

    def fetch_single_page(self, page: int, page_size: int = 20, 
                          display_category_id: Optional[str] = None, 
                          sort_type: Optional[str] = None,
                          collection_timestamp: Optional[str] = None) -> List[Dict]:
        """
        ë‹¨ì¼ í˜ì´ì§€ì˜ ìƒí’ˆ ì •ë³´ë¥¼ GraphQL APIë¥¼ í†µí•´ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        """
        # ê¸°ë³¸ê°’ ì‚¬ìš© ë˜ëŠ” ì¸ìê°’ ì˜¤ë²„ë¼ì´ë“œ
        if display_category_id is None:
            display_category_id = self.default_display_category_id
        if sort_type is None:
            sort_type = self.default_sort_type

        # POST ìš”ì²­ì— í•„ìš”í•œ í˜ì´ë¡œë“œ ìƒì„±
        payload = self._build_graphql_payload(page, page_size, display_category_id, sort_type)
        
        try:
            # POST ìš”ì²­ìœ¼ë¡œ GraphQL API í˜¸ì¶œ, json=payload ì‚¬ìš©
            response = self.session.post(self.base_api_url, json=payload, timeout=30)
            response.raise_for_status() # HTTP ì˜¤ë¥˜(4xx, 5xx) ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
            
            data = response.json()
            
            # --- ë””ë²„ê¹… ì¶”ê°€: ì‘ë‹µ ë°ì´í„°ê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸ ---
            if not isinstance(data, dict):
                logger.error(f"í˜ì´ì§€ {page} ì‘ë‹µì´ ìœ íš¨í•œ JSON ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤. ìˆ˜ì§‘ ì‹¤íŒ¨. ì‘ë‹µ í…ìŠ¤íŠ¸: {response.text[:500]}...") # ë¶€ë¶„ ì¶œë ¥
                return []
            # --- ë””ë²„ê¹… ì¶”ê°€ ë ---

            card_items = data.get('data', {}).get('pagedCards', {}).get('data', [])
            
            products = []
            for card in card_items:
                # _parse_product_card ë©”ì„œë“œì— ìˆ˜ì§‘ ë‚ ì§œ ì „ë‹¬
                product = self._parse_product_card(card, collection_timestamp)
                if product:
                    products.append(product)
            
            # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€: API ì‘ë‹µì—ì„œ íŒŒì‹±ëœ ìƒí’ˆ ì¹´ë“œ ê°œìˆ˜ì™€ ìµœì¢… ì¶”ì¶œëœ ìƒí’ˆ ê°œìˆ˜ í™•ì¸
            logger.info(f"  í˜ì´ì§€ {page}: API ì‘ë‹µì—ì„œ {len(card_items)}ê°œ ìƒí’ˆ ì¹´ë“œ í™•ì¸, ìµœì¢… {len(products)}ê°œ ìƒí’ˆ ì¶”ì¶œë¨.")
            return products
        
        except requests.exceptions.RequestException as e:
            logger.error(f"í˜ì´ì§€ {page} API ìš”ì²­ ì¤‘ ë„¤íŠ¸ì›Œí¬/HTTP ì˜¤ë¥˜ ë°œìƒ: {e}. ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code if 'response' in locals() else 'N/A'}")
            return []
        except json.JSONDecodeError as e:
            logger.error(f"í˜ì´ì§€ {page} JSON ì‘ë‹µ ë””ì½”ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì‘ë‹µ í…ìŠ¤íŠ¸ (ë¶€ë¶„): {response.text[:500] if 'response' in locals() else 'N/A'}")
            return []
        except KeyError as e:
            logger.error(f"í˜ì´ì§€ {page} ì‘ë‹µ ë°ì´í„° êµ¬ì¡° ì˜¤ë¥˜: í•„ìˆ˜ í‚¤ ì—†ìŒ - {e}. ì‘ë‹µ: {data if 'data' in locals() else 'N/A'}")
            return []
        except Exception as e:
            logger.error(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return []

    def fetch_products_api(self, max_pages: int, page_size: int = 20, 
                           display_category_id: Optional[str] = None, 
                           sort_type: Optional[str] = None,
                           collection_timestamp: Optional[str] = None) -> List[Dict]:
        """
        GraphQL APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì§€ì •ëœ ì¹´í…Œê³ ë¦¬ì—ì„œ ì—¬ëŸ¬ í˜ì´ì§€ì˜ ìƒí’ˆ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        """
        if display_category_id is None:
            display_category_id = self.default_display_category_id
        if sort_type is None:
            sort_type = self.default_sort_type

        all_products = []
        for page in range(1, max_pages + 1):
            logger.info(f"âœ¨ ì¹´í…Œê³ ë¦¬ '{display_category_id}' - {page}í˜ì´ì§€ ìƒí’ˆ ìˆ˜ì§‘ ì¤‘...")
            # fetch_single_page ë©”ì„œë“œì— ìˆ˜ì§‘ ë‚ ì§œ ì „ë‹¬
            products_on_page = self.fetch_single_page(page, page_size, display_category_id, sort_type, collection_timestamp)
            
            if not products_on_page:
                logger.info(f"í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë¹ˆ ì‘ë‹µ ë˜ëŠ” ì˜¤ë¥˜ ë°œìƒ). ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break # ë” ì´ìƒ ìƒí’ˆì´ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ ë£¨í”„ ì¢…ë£Œ
            
            all_products.extend(products_on_page)
            time.sleep(2.0) # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸° ì‹œê°„ (2.0ì´ˆ)

        return all_products

    def _parse_product_card(self, card: Dict, collection_timestamp: Optional[str] = None) -> Optional[Dict]:
        """
        ë‹¨ì¼ ìƒí’ˆ ì¹´ë“œ ë”•ì…”ë„ˆë¦¬ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ìˆ˜ì§‘ ë‚ ì§œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        'product' í•„ë“œê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ Noneì¸ ê²½ìš°ë¥¼ ë°©ì–´ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        product_data_from_card = card.get('data', {}) # 'data' í‚¤ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬
        
        # 'data' í•„ë“œê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆë©´ ìœ íš¨í•˜ì§€ ì•Šì€ ì¹´ë“œì´ë¯€ë¡œ ê±´ë„ˆëœ€
        if not isinstance(product_data_from_card, dict):
            logger.warning(f"ì¹´ë“œ 'data' í•„ë“œê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(product_data_from_card)} for card {card.get('cardId', 'N/A')}. ìŠ¤í‚µ.")
            return None

        product = product_data_from_card.get('product', None) 

        # 'product'ê°€ Noneì´ê±°ë‚˜ ë”•ì…”ë„ˆë¦¬ í˜•íƒœê°€ ì•„ë‹ˆë©´ ìœ íš¨í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨
        if product is None or not isinstance(product, dict):
            logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ 'product' ë°ì´í„° êµ¬ì¡° ë°œê²¬ (ìœ í˜•: {type(product)} ë˜ëŠ” None) for card {card.get('cardId', 'N/A')}. ìŠ¤í‚µ.")
            return None
        
        # product ë”•ì…”ë„ˆë¦¬ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ({}ì¸ ê²½ìš°)
        if not product: 
            logger.warning("ìƒí’ˆ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ê°€ ë¹„ì–´ìˆìŒ for card: %s. ìŠ¤í‚µ.", card.get('cardId', 'N/A'))
            return None
        
        # ì—¬ê¸°ê¹Œì§€ ë„ë‹¬í–ˆë‹¤ë©´ productëŠ” ìœ íš¨í•œ (ë¹„ì–´ìˆì§€ ì•Šì€) ë”•ì…”ë„ˆë¦¬ì„ì´ ë³´ì¥ë©ë‹ˆë‹¤.
        # ê¸°ì¡´ íŒŒì‹± ë¡œì§ ê³„ì†.
        # ê°€ê²© ì •ë³´: salePrice(í• ì¸ ê°€ê²©) ìš°ì„ , ì—†ìœ¼ë©´ originalPrice(ì •ê°€) ì‚¬ìš©
        price = product.get('salePrice', product.get('originalPrice', 'N/A'))
        
        # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ì¶œ ë¡œì§
        category_names = []
        categories_list_from_product = product.get('categories', []) # 'categories' í‚¤ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‹œë„
        
        if categories_list_from_product:
            # 'categories' ë¦¬ìŠ¤íŠ¸ê°€ ì¡´ì¬í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ê±°ê¸°ì„œ ì´ë¦„ ì¶”ì¶œ
            category_names = [cat.get('name') for cat in categories_list_from_product if cat.get('name')]
            if not category_names: # ë¦¬ìŠ¤íŠ¸ëŠ” ìˆì—ˆìœ¼ë‚˜ ì´ë¦„ì´ ëª¨ë‘ None/ë¹„ì–´ìˆì—ˆë˜ ê²½ìš°
                logger.warning(f"ìƒí’ˆ ID {product.get('id', 'N/A')}ì˜ 'categories' ë¦¬ìŠ¤íŠ¸ëŠ” ì¡´ì¬í•˜ì§€ë§Œ ìœ íš¨í•œ ì´ë¦„ì´ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸: {categories_list_from_product}")
        
        # 'categories' ë¦¬ìŠ¤íŠ¸ì—ì„œ ìœ íš¨í•œ ì´ë¦„ì„ ì–»ì§€ ëª»í–ˆê³ , 'productCategoryName'ì´ ì¡´ì¬í•˜ë©´ ê·¸ê±¸ ì‚¬ìš©
        if not category_names and product.get('productCategoryName'):
            category_names = [product.get('productCategoryName')]
            # logger.info(f"ìƒí’ˆ ID {product.get('id', 'N/A')}ì— ëŒ€í•´ 'productCategoryName' ì‚¬ìš©: {category_names[0]}") # ë„ˆë¬´ ë§ì€ ë¡œê·¸ ë°©ì§€
        
        # ë‘ ê°€ì§€ ëª¨ë‘ ì—†ëŠ” ê²½ìš°
        if not category_names:
            logger.warning(f"ìƒí’ˆ ID {product.get('id', 'N/A')}ì— ëŒ€í•œ ì¹´í…Œê³ ë¦¬ ë°ì´í„°(categories ë˜ëŠ” productCategoryName)ê°€ ëª¨ë‘ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
       
        # ë¸Œëœë“œ ì •ë³´ ì¶”ì¶œ
        brand = (product.get('brand') or {}).get('name', 'N/A')
        
        # ìƒí’ˆ ìƒì„¸ URL ìƒì„±
        sub_vertical = product.get('channel', {}).get('subVertical')
        product_id = product.get('id')

        if sub_vertical and product_id:
            product_url = f"https://shopping.naver.com/window-products/{sub_vertical.lower()}/{product_id}"
        else:
            logger.warning(f"URL ìƒì„± ì‹¤íŒ¨: id={product_id}, subVertical={sub_vertical}")
            product_url = 'N/A'

        # ë¦¬ë·° ìˆ˜ì™€ í‰ì  í•„ë“œëª… ë³€ê²½
        review_count = product.get('totalReviewCount', 0) # í•„ë“œëª… ë³€ê²½
        avg_review_score = product.get('averageReviewScore', 0.0) # í•„ë“œëª… ë³€ê²½

        # ë””ë²„ê¹… ì¶”ê°€: ë¦¬ë·°/í‰ì  ë°ì´í„°ê°€ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©ë  ê²½ìš° ì›ë³¸ product ë°ì´í„° ë¡œê¹…
        if (review_count == 0 and avg_review_score == 0.0) and \
           (product.get('totalReviewCount') is None or product.get('averageReviewScore') is None):
             logger.warning(f"ìƒí’ˆ ID {product.get('id', 'N/A')}ì— ëŒ€í•œ ë¦¬ë·°/í‰ì  ë°ì´í„°ê°€ ëˆ„ë½ë˜ì–´ ê¸°ë³¸ê°’ ì‚¬ìš©. ì›ë³¸ product ë°ì´í„° (ì¼ë¶€): {json.dumps(product, ensure_ascii=False, indent=2)[:500]}...") # ë¡œê·¸ ê¸¸ì´ ì œí•œ
        
        parsed_data = {
            "product_id": product_id,
            "name": product.get('name', 'N/A'),
            "brand": brand,
            "price": price,
            "originalPrice": product.get('originalPrice', 'N/A'),
            "representativeImageUrl": product.get('representativeImageUrl', 'N/A'),
            "reviewCount": review_count, # ì¶”ì¶œëœ ë³€ìˆ˜ ì‚¬ìš©
            "avgReviewScore": avg_review_score, # ì¶”ì¶œëœ ë³€ìˆ˜ ì‚¬ìš©
            "product_url": product_url,
            "categories": category_names, # ìˆ˜ì •ëœ category_names ë³€ìˆ˜ ì‚¬ìš©
            "detailContent": product.get('detailContent', 'N/A'), # ìƒí’ˆ ìƒì„¸ ì„¤ëª… (HTML/í…ìŠ¤íŠ¸ ë“±)
            "channelName": product.get('channel', {}).get('name', 'N/A'), # íŒë§¤ ì±„ë„ëª…
        }
        
        # ìˆ˜ì§‘ ë‚ ì§œ í•„ë“œ ì¶”ê°€
        if collection_timestamp:
            parsed_data["collectionDate"] = collection_timestamp
            
        return parsed_data

    def save_to_csv(self, data: List[Dict], filename: str):
        """ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not data:
            logger.warning(f"'{filename}'ì— ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        try:
            df = pd.DataFrame(data)
            df.to_csv(filename, index=False, encoding='utf-8-sig') # í•œê¸€ ê¹¨ì§ ë°©ì§€ë¥¼ ìœ„í•´ 'utf-8-sig' ì‚¬ìš©
            logger.info(f"ë°ì´í„°ê°€ '{filename}'ì— CSV í˜•ì‹ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ì´ {len(data)}ê°œ)")
        except Exception as e:
            logger.error(f"CSV ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

    def save_to_json(self, data: List[Dict], filename: str):
        """ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if not data:
            logger.warning(f"'{filename}'ì— ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4) # í•œê¸€ ì¸ì½”ë”© ë° ê°€ë…ì„±ì„ ìœ„í•´ ensure_ascii=False, indent=4
            logger.info(f"ë°ì´í„°ê°€ '{filename}'ì— JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ì´ {len(data)}ê°œ)")
        except Exception as e:
            logger.error(f"JSON ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

def main_collect_products_by_categories():
    """
    ì§€ì •ëœ ì¹´í…Œê³ ë¦¬ë“¤ì˜ ìƒí’ˆì„ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    
    # ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    data_dir = "collected_data"
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
    
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = NaverShoppingCrawler()
    
    # --- ìˆ˜ì§‘í•˜ê³ ì í•˜ëŠ” ë„¤ì´ë²„ ë·°í‹° ì¹´í…Œê³ ë¦¬ ID ë¦¬ìŠ¤íŠ¸ ---
    # ì˜ˆì‹œ: ë„¤ì´ë²„ ë·°í‹° ì „ì²´(20006491), ìŠ¤í‚¨ì¼€ì–´(20006492), í´ë Œì§•(20006509)
    target_category_ids = {
        "ë·°í‹°_ì „ì²´": "20006491",
        "ìŠ¤í‚¨ì¼€ì–´": "20006492",
        "ì„ ì¼€ì–´": "20006505",
        "ë§ˆìŠ¤í¬_íŒ©": "20006513", # íŒŒì¼ëª…ì— ì‚¬ìš©ë  ìˆ˜ ìˆë„ë¡ ìŠ¬ë˜ì‹œ ëŒ€ì‹  ì–¸ë”ìŠ¤ì½”ì–´
        "í´ë Œì§•": "20006522",
        "ë©”ì´í¬ì—…": "20006536",
        "ë„¤ì¼ì¼€ì–´": "20006567",
        "ë°”ë””ì¼€ì–´": "20006585",
        "í—¤ì–´ì¼€ì–´": "20006612",
        "ë·°í‹°ì†Œí’ˆ": "20006651",
        "í–¥ìˆ˜": "20006681",
        "ë‚¨ì„±í™”ì¥í’ˆ": "20006688",
        "ë·°í‹°ë””ë°”ì´ìŠ¤": "20006713",
        "ìœ ì•„ë™í™”ì¥í’ˆ": "20006719"
    }
    # ----------------------------------------------------

    # ìˆ˜ì§‘ ëª©í‘œ: ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì¸ê¸°ìˆœ 500ê°œ ë°ì´í„°
    items_per_category = 500
    page_size = 20 # í•œ í˜ì´ì§€ì— 20ê°œ ìƒí’ˆì”© ê°€ì ¸ì˜´ (API ê¸°ë³¸)
    max_pages_to_fetch = (items_per_category + page_size - 1) // page_size 

    logger.info(f"ğŸš€ ì´ {len(target_category_ids)}ê°œì˜ ì¹´í…Œê³ ë¦¬ì—ì„œ ê°ê° ì•½ {items_per_category}ê°œ ìƒí’ˆ ìˆ˜ì§‘ ì‹œì‘ (í˜ì´ì§€ë‹¹ {page_size}ê°œ, ìµœëŒ€ {max_pages_to_fetch}í˜ì´ì§€).")

    # ìˆ˜ì§‘ ì‹œì‘ ì‹œê°„ ê¸°ë¡ (íŒŒì¼ ì´ë¦„ ë° ë°ì´í„° ë‚´ í•„ë“œì— ì‚¬ìš©)
    current_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = os.path.join(data_dir, current_timestamp)
    os.makedirs(save_dir, exist_ok=True)

    for category_name, category_id in target_category_ids.items():
        try:
            logger.info(f"\n--- ì¹´í…Œê³ ë¦¬ '{category_name}' (ID: {category_id}) ìƒí’ˆ ìˆ˜ì§‘ ì‹œì‘ ---")
            
            products_for_category = crawler.fetch_products_api(
                display_category_id=category_id,
                sort_type="DISPLAY_CATEGORY_GENDER_AGE_GROUP_F20", # ì¸ê¸°ìˆœ ì •ë ¬ ê³ ì •
                max_pages=max_pages_to_fetch, 
                page_size=page_size,
                collection_timestamp=current_timestamp # ìˆ˜ì§‘ ë‚ ì§œ/ì‹œê°„ ì „ë‹¬
            )

            if not products_for_category:
                logger.warning(f"ì¹´í…Œê³ ë¦¬ '{category_name}'ì—ì„œ ìƒí’ˆì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
                continue 
            
            # ìˆ˜ì§‘ëœ ìƒí’ˆ ê°œìˆ˜ í™•ì¸ (ì •í™•íˆ 500ê°œê°€ ì•„ë‹ ìˆ˜ ìˆìŒ, API ì‘ë‹µì— ë”°ë¼ ë‹¬ë¼ì§)
            actual_items_collected = len(products_for_category)
            logger.info(f"âœ… ì¹´í…Œê³ ë¦¬ '{category_name}' í¬ë¡¤ë§ ì™„ë£Œ! ì´ {actual_items_collected}ê°œ ìƒí’ˆ ìˆ˜ì§‘.")
            
            # íŒŒì¼ ì´ë¦„ êµ¬ì„±
            csv_file = os.path.join(save_dir, f"{category_name}_{category_id}.csv")
            json_file = os.path.join(save_dir, f"{category_name}_{category_id}.json")

            crawler.save_to_csv(products_for_category, csv_file)
            crawler.save_to_json(products_for_category, json_file)
            
            # ìˆ˜ì§‘ëœ ìƒí’ˆ ìƒ˜í”Œ ì¶œë ¥ (1ê°œë§Œ ì¶œë ¥)
            print(f"\n=== ì¹´í…Œê³ ë¦¬ '{category_name}' ìˆ˜ì§‘ ìƒí’ˆ ìƒ˜í”Œ (ìƒìœ„ 1ê°œ) ===")
            for i, product in enumerate(products_for_category[:1], 1):
                print(f"\n{i}. {product.get('name', 'N/A')}")
                print(f"   ë¸Œëœë“œ: {product.get('brand', 'N/A')}")
                print(f"   ê°€ê²©: {product.get('price', 'N/A')}ì›")
                print(f"   ë¦¬ë·° ìˆ˜: {product.get('reviewCount', 0)} (í‰ì : {product.get('avgReviewScore', 0.0)})")
                print(f"   URL: {product.get('product_url', 'N/A')}")
                print(f"   ì¹´í…Œê³ ë¦¬: {', '.join(product.get('categories', []))}")
                print(f"   ìˆ˜ì§‘ì¼: {product.get('collectionDate', 'N/A')}") # ì¶”ê°€ëœ í•„ë“œ ì¶œë ¥
            
            time.sleep(3) # ê° ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ í›„ ì¶©ë¶„í•œ ëŒ€ê¸° ì‹œê°„
            
        except Exception as e:
            logger.error(f"ì¹´í…Œê³ ë¦¬ '{category_name}' ìˆ˜ì§‘ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

    logger.info("ëª¨ë“  ì§€ì • ì¹´í…Œê³ ë¦¬ ìƒí’ˆ ìˆ˜ì§‘ ì™„ë£Œ!")


# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    main_collect_products_by_categories()
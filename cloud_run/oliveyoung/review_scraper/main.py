"""
ì˜¬ë¦¬ë¸Œì˜ ë¦¬ë·° í¬ë¡¤ëŸ¬ - Cloud Run Job
íŠ¹ì • ìƒí’ˆì˜ ë¦¬ë·° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ GCSì— ì €ì¥
"""

import os
import sys
import json
import logging
from datetime import datetime, timezone
from oliveyoung_review_scraper_module import OliveYoungReviewScraper
from gcs_uploader import GCSUploader

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def scrape_reviews(product_id: str, product_url: str, max_pages: int = 1):
    """ë¦¬ë·° í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
    try:
        product_name = ""

        logger.info(f"ğŸŸ¢ Start scraping: product_id={product_id}, max_pages={max_pages}")
        logger.info(f"URL: {product_url}")

        scraper = OliveYoungReviewScraper()

        try:
            crawling_started_at = datetime.now(timezone.utc).isoformat()
            crawling_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")

            # ë¦¬ë·° í¬ë¡¤ë§
            reviews, category_name = scraper.extract_reviews_with_pagination(product_url, max_pages)

            if not reviews:
                logger.warning(f"No reviews found for product_id={product_id}")
                return {
                    "status": "completed",
                    "product_id": product_id,
                    "product_name": product_name,
                    "reviews_count": 0,
                    "message": "No reviews found"
                }

            # ì²« ë¦¬ë·°ì—ì„œ ìƒí’ˆëª… ì¶”ì¶œ
            if 'product_name' in reviews[0]:
                product_name = reviews[0]['product_name']

            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for review in reviews:
                review.update({
                    "product_id": product_id,
                    "product_name": product_name,
                    "product_url": product_url,
                    "category_name": category_name,
                    "crawling_timestamp": crawling_started_at,
                    "crawling_date": crawling_date,
                    "source": "oliveyoung",
                    "data_type": "reviews"
                })

            # GCS ì—…ë¡œë“œ
            gcs_uploader = GCSUploader()
            now = datetime.now(timezone.utc)
            year, month, day = now.strftime("%Y"), now.strftime("%m"), now.strftime("%d")
            timestamp = now.strftime("%Y%m%d_%H%M%S")
            file_path = f"raw-data/olive-young/reviews/{product_id}/{year}/{month}/{day}/{product_id}_{timestamp}.csv"

            upload_result = gcs_uploader.upload_csv(reviews, file_path)

            logger.info(f"âœ… Scraped {len(reviews)} reviews for {product_name}")
            logger.info(f"ğŸ“¦ Uploaded to: {upload_result['gcs_path']}")

            return {
                "status": "completed",
                "product_id": product_id,
                "product_name": product_name,
                "reviews_count": len(reviews),
                "gcs_path": upload_result['gcs_path']
            }

        finally:
            scraper.close()

    except Exception as e:
        logger.error(f"Error during scraping: {str(e)}", exc_info=True)
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }


def main():
    """í™˜ê²½ë³€ìˆ˜ë¡œ ì‹¤í–‰ (Cloud Run í˜¸í™˜)"""
    try:
        # í™˜ê²½ ë³€ìˆ˜ë¡œë¶€í„° ì¸ì ë¡œë“œ
        product_id = os.environ["PRODUCT_ID"]
        product_url = os.environ["PRODUCT_URL"]
        max_pages = int(os.environ.get("MAX_PAGES", "1"))

        result = scrape_reviews(product_id, product_url, max_pages)

        print(json.dumps(result, indent=2, ensure_ascii=False))

        if result["status"] == "completed":
            sys.exit(0)
        else:
            sys.exit(1)

    except KeyError as e:
        logger.error(f"í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ ëˆ„ë½: {str(e)}", exc_info=True)
        print(json.dumps({
            "status": "error",
            "error": f"Missing env var: {str(e)}",
            "timestamp": datetime.now(timezone.utc).isoformat()
        }, indent=2, ensure_ascii=False))
        sys.exit(1)

    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}", exc_info=True)
        print(json.dumps({
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }, indent=2, ensure_ascii=False))
        sys.exit(1)


if __name__ == "__main__":
    main()

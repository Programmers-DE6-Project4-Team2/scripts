import os
import sys
import logging
from datetime import datetime, timezone
import pandas as pd
from google.cloud import storage
from oliveyoung_crawler_module import OliveYoungProductCrawler
from dotenv import load_dotenv

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
load_dotenv()


def upload_csv_to_gcs(bucket_name: str, dataframe: pd.DataFrame, destination_blob_name: str):
    logger.info("ğŸ”„ [GCS] ì—…ë¡œë“œ ì‹œì‘...")
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)
    csv_data = dataframe.to_csv(index=False, encoding="utf-8-sig")
    logger.info(f"ğŸ”„ [GCS] CSV ë³€í™˜ ì™„ë£Œ, í¬ê¸°: {len(csv_data)} bytes")
    blob.upload_from_string(csv_data, content_type="text/csv")
    logger.info(f"âœ… [GCS] ì—…ë¡œë“œ ì™„ë£Œ: gs://{bucket_name}/{destination_blob_name}")
    return f"gs://{bucket_name}/{destination_blob_name}"


def main():
    # ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
    if len(sys.argv) >= 4:
        category_name = sys.argv[1]
        category_url = sys.argv[2]
        max_pages = int(sys.argv[3])
    else:
        # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (fallback)
        category_url = os.getenv("CATEGORY_URL")
        category_name = os.getenv("CATEGORY_NAME", "default")
        max_pages = int(os.getenv("MAX_PAGES", "1"))

    bucket_name = os.getenv("BUCKET_NAME", "de6-ez2")

    if not category_url:
        raise ValueError("CATEGORY_URLì´ í•„ìš”í•©ë‹ˆë‹¤. ëª…ë ¹í–‰ ì¸ìˆ˜ ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì œê³µí•˜ì„¸ìš”.")

    logger.info(f"[START] category={category_name} max_pages={max_pages}")
    crawler = OliveYoungProductCrawler(headless=True)

    try:
        # 1. í¬ë¡¤ë§
        products = crawler.extract_product_list(category_url, max_pages=max_pages)
        df = pd.DataFrame(products)

        # 2. GCS ì—…ë¡œë“œ
        now = datetime.now(timezone.utc)
        timestamp = now.strftime("%Y%m%d_%H%M%S")
        year = now.strftime("%Y")
        month = now.strftime("%m")
        day = now.strftime("%d")
        filename = f"raw-data/olive-young/products/{category_name}/{year}/{month}/{day}/{category_name}_{timestamp}.csv"
        gcs_path = upload_csv_to_gcs(bucket_name, df, filename)

        # 3. ì™„ë£Œ ë¡œê·¸
        logger.info(f"ğŸ‰ ì‘ì—… ì™„ë£Œ: {len(products)}ê°œ ìƒí’ˆ, ê²½ë¡œ: {gcs_path}")

    except Exception as e:
        logger.exception("í¬ë¡¤ë§ ë„ì¤‘ ì—ëŸ¬ ë°œìƒ")
        raise

    finally:
        crawler.close()

if __name__ == "__main__":
    main()

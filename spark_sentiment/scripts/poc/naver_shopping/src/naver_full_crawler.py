#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ì‡¼í•‘ ìŠ¤í‚¨ì¼€ì–´ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘
"""
from naver_shopping_crawler import NaverShoppingCrawler
from datetime import datetime
import logging
import os

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def collect_all_skincare_products():
    """ìŠ¤í‚¨ì¼€ì–´ ì „ì²´ ìƒí’ˆ ìˆ˜ì§‘"""
    
    # ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    data_dir = "data"
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = NaverShoppingCrawler()
    
    try:
        logger.info("ğŸš€ ë„¤ì´ë²„ ì‡¼í•‘ ìŠ¤í‚¨ì¼€ì–´ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        
        # ìµœëŒ€ 50í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘ (ì•½ 1,000ê°œ ìƒí’ˆ)
        all_products = []
        batch_size = 10  # 10í˜ì´ì§€ì”© ë°°ì¹˜ ì²˜ë¦¬
        
        for start_page in range(1, 51, batch_size):
            end_page = min(start_page + batch_size - 1, 50)
            
            logger.info(f"ğŸ“¦ ë°°ì¹˜ ìˆ˜ì§‘: {start_page}í˜ì´ì§€ ~ {end_page}í˜ì´ì§€")
            
            # ë°°ì¹˜ë³„ ìƒí’ˆ ìˆ˜ì§‘
            batch_products = []
            for page in range(start_page, end_page + 1):
                try:
                    page_products = crawler.fetch_single_page(page)
                    if page_products:
                        batch_products.extend(page_products)
                        logger.info(f"  âœ… í˜ì´ì§€ {page}: {len(page_products)}ê°œ ìƒí’ˆ")
                    else:
                        logger.warning(f"  âŒ í˜ì´ì§€ {page}: ë°ì´í„° ì—†ìŒ (ìˆ˜ì§‘ ì¢…ë£Œ)")
                        break
                        
                except Exception as e:
                    logger.error(f"  âŒ í˜ì´ì§€ {page} ì˜¤ë¥˜: {e}")
                    break
            
            if not batch_products:
                logger.info("ë” ì´ìƒ ìˆ˜ì§‘í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            # ë°°ì¹˜ ê²°ê³¼ë¥¼ ì „ì²´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            all_products.extend(batch_products)
            
            # ì¤‘ê°„ ì €ì¥ (ë°ì´í„° ìœ ì‹¤ ë°©ì§€)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            temp_file = f"{data_dir}/naver_skincare_temp_{timestamp}.json"
            crawler.save_to_json(all_products, temp_file)
            
            logger.info(f"ğŸ“Š í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘: {len(all_products)}ê°œ ìƒí’ˆ (ì¤‘ê°„ì €ì¥: {temp_file})")
            
            # ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
            import time
            time.sleep(2)
        
        if not all_products:
            logger.error("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ìµœì¢… ì €ì¥
        final_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # CSV ì €ì¥
        csv_file = f"{data_dir}/naver_skincare_all_{final_timestamp}.csv"
        crawler.save_to_csv(all_products, csv_file)
        
        # JSON ì €ì¥
        json_file = f"{data_dir}/naver_skincare_all_{final_timestamp}.json"
        crawler.save_to_json(all_products, json_file)
        
        # ê²°ê³¼ í†µê³„
        logger.info("ğŸ‰ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        logger.info(f"ğŸ“ˆ ì´ ìˆ˜ì§‘ ìƒí’ˆ: {len(all_products):,}ê°œ")
        logger.info(f"ğŸ’¾ ì €ì¥ íŒŒì¼: {csv_file}, {json_file}")
        
        # ë¸Œëœë“œë³„ í†µê³„
        brands = {}
        price_ranges = {"1ë§Œì› ë¯¸ë§Œ": 0, "1-3ë§Œì›": 0, "3-5ë§Œì›": 0, "5ë§Œì› ì´ìƒ": 0}
        
        for product in all_products:
            # ë¸Œëœë“œ í†µê³„
            brand = product.get('brand', 'Unknown')
            brands[brand] = brands.get(brand, 0) + 1
            
            # ê°€ê²©ëŒ€ í†µê³„
            try:
                price = int(product.get('price', 0))
                if price < 10000:
                    price_ranges["1ë§Œì› ë¯¸ë§Œ"] += 1
                elif price < 30000:
                    price_ranges["1-3ë§Œì›"] += 1
                elif price < 50000:
                    price_ranges["3-5ë§Œì›"] += 1
                else:
                    price_ranges["5ë§Œì› ì´ìƒ"] += 1
            except:
                pass
        
        # ìƒìœ„ ë¸Œëœë“œ ì¶œë ¥
        top_brands = sorted(brands.items(), key=lambda x: x[1], reverse=True)[:10]
        
        print("\n" + "="*50)
        print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ í†µê³„")
        print("="*50)
        print(f"ì´ ìƒí’ˆ ìˆ˜: {len(all_products):,}ê°œ")
        print(f"ë¸Œëœë“œ ìˆ˜: {len(brands)}ê°œ")
        
        print("\nğŸ† ìƒìœ„ ë¸Œëœë“œ (ìƒí’ˆ ìˆ˜):")
        for brand, count in top_brands:
            print(f"  {brand}: {count}ê°œ")
        
        print("\nğŸ’° ê°€ê²©ëŒ€ë³„ ë¶„í¬:")
        for price_range, count in price_ranges.items():
            percentage = (count / len(all_products)) * 100
            print(f"  {price_range}: {count}ê°œ ({percentage:.1f}%)")
        
    except Exception as e:
        logger.error(f"ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# NaverShoppingCrawler í´ë˜ìŠ¤ì— ë‹¨ì¼ í˜ì´ì§€ ìˆ˜ì§‘ ë©”ì„œë“œ ì¶”ê°€
def fetch_single_page(self, page: int) -> list:
    """ë‹¨ì¼ í˜ì´ì§€ ìƒí’ˆ ìˆ˜ì§‘"""
    try:
        url = self.build_api_url(page)
        response = self.session.get(url, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        
        if 'data' in data and 'pagedCards' in data['data']:
            cards = data['data']['pagedCards'].get('data', [])
            
            products = []
            for card in cards:
                product = self.parse_product_card(card)
                if product:
                    products.append(product)
            
            return products
        return []
        
    except Exception as e:
        logging.getLogger(__name__).error(f"í˜ì´ì§€ {page} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return []

# ë©”ì„œë“œë¥¼ í´ë˜ìŠ¤ì— ë™ì ìœ¼ë¡œ ì¶”ê°€
NaverShoppingCrawler.fetch_single_page = fetch_single_page

if __name__ == "__main__":
    collect_all_skincare_products()
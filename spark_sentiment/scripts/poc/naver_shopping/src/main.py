#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ì‡¼í•‘ í¬ë¡¤ëŸ¬ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""
import os
import sys
from datetime import datetime
from naver_shopping_crawler import NaverShoppingCrawler
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ›’ ë„¤ì´ë²„ ì‡¼í•‘ í¬ë¡¤ëŸ¬")
    print("=" * 40)
    
    # ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    data_dir = "../data"
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
    
    # ì‚¬ìš©ì ì˜µì…˜ ì„ íƒ
    print("ì‹¤í–‰í•  ì‘ì—…ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ê¸°ë³¸ í¬ë¡¤ë§ (100ê°œ ìƒí’ˆ)")
    print("2. ì „ì²´ í¬ë¡¤ë§ (1,000ê°œ ìƒí’ˆ)")
    print("3. ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„")
    print("4. API ì‘ë‹µ í…ŒìŠ¤íŠ¸")
    print("5. ì •ë ¬ ì˜µì…˜ ë¶„ì„")
    
    choice = input("\nì„ íƒ (1-5): ").strip()
    
    if choice == "1":
        basic_crawling(data_dir)
    elif choice == "2":
        full_crawling(data_dir)
    elif choice == "3":
        site_analysis()
    elif choice == "4":
        api_test()
    elif choice == "5":
        sort_analysis()
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

def basic_crawling(data_dir):
    """ê¸°ë³¸ í¬ë¡¤ë§ (100ê°œ ìƒí’ˆ)"""
    print("\nğŸš€ ê¸°ë³¸ í¬ë¡¤ë§ ì‹œì‘ (5í˜ì´ì§€, 100ê°œ ìƒí’ˆ)")
    
    crawler = NaverShoppingCrawler()
    
    try:
        # 5í˜ì´ì§€ í¬ë¡¤ë§
        products = crawler.fetch_products_api(max_pages=5)
        
        if not products:
            logger.error("ìƒí’ˆ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ë°ì´í„° ì €ì¥
        csv_file = f"{data_dir}/naver_basic_{timestamp}.csv"
        json_file = f"{data_dir}/naver_basic_{timestamp}.json"
        
        crawler.save_to_csv(products, csv_file)
        crawler.save_to_json(products, json_file)
        
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“Š ìˆ˜ì§‘ ìƒí’ˆ: {len(products)}ê°œ")
        print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {csv_file}, {json_file}")
        
        # ìƒìœ„ 3ê°œ ìƒí’ˆ ë¯¸ë¦¬ë³´ê¸°
        print("\nğŸ“¦ ìˆ˜ì§‘ëœ ìƒí’ˆ ë¯¸ë¦¬ë³´ê¸°:")
        for i, product in enumerate(products[:3], 1):
            print(f"{i}. {product.get('name', 'N/A')[:50]}...")
            print(f"   ğŸ’° {product.get('price', 'N/A')}ì› â­ {product.get('rating', 'N/A')}")
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def full_crawling(data_dir):
    """ì „ì²´ í¬ë¡¤ë§ (1,000ê°œ ìƒí’ˆ)"""
    print("\nğŸ”¥ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ (50í˜ì´ì§€, 1,000ê°œ ìƒí’ˆ)")
    print("â° ì˜ˆìƒ ì†Œìš”ì‹œê°„: ì•½ 5ë¶„")
    
    confirm = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
    if confirm != 'y':
        print("ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    # naver_full_crawler.py ì‹¤í–‰
    os.system("python naver_full_crawler.py")

def site_analysis():
    """ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„"""
    print("\nğŸ” ë„¤ì´ë²„ ì‡¼í•‘ ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„")
    print("ë¸Œë¼ìš°ì €ê°€ ì—´ë¦¬ê³  í˜ì´ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...")
    
    # naver_analysis.py ì‹¤í–‰
    os.system("python naver_analysis.py")

def api_test():
    """API ì‘ë‹µ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª ë„¤ì´ë²„ ì‡¼í•‘ API ì‘ë‹µ êµ¬ì¡° í…ŒìŠ¤íŠ¸")
    
    # test_naver_api.py ì‹¤í–‰
    os.system("python test_naver_api.py")

def sort_analysis():
    """ì •ë ¬ ì˜µì…˜ ë¶„ì„"""
    print("\nğŸ“ˆ ë‹¤ì–‘í•œ ì •ë ¬ ì˜µì…˜ ë¶„ì„")
    print("ì¸ê¸°ìˆœ, ë¦¬ë·°ìˆœ, ìµœì‹ ìˆœ ë“±ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤...")
    
    # check_sort_options.py ì‹¤í–‰
    os.system("python check_sort_options.py")

if __name__ == "__main__":
    main()